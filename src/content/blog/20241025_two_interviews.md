---
title: 'Two Interviews: Geoff Hinton and Yann LeCun'
slug: 'two-interviews'
description: 'Discussion of interviews with Geoff Hinton and Yann LeCun'
pubDate: 'October 25, 2024'
---

After the Nobel Prize in Physics was awarded to Geoff Hinton, Adam Smith from NobelPrize.org called Professor Hinton to discuss his achievement. The full discussion is [here](https://www.nobelprize.org/prizes/physics/2024/hinton/interview/).

During the interview, Prof. Hinton expresses his fears of artificial intelligence. Like many, he's deeply concerned about the future of machine learning but he doesn't mention any specific threats. One point that I found interesting is his belief that large language models (LLMs) truly grasp the meaning of the text they process: "these things really do understand what they’re saying".

Two weeks ago, the Wall Street Journal interviewed Yann LeCunn and asked for his opinions (the archived article is [here](https://archive.ph/ooEEY)). On every topic, Prof. LeCunn's opinions are diametrically opposed to Prof. Hinton's. Prof. LeCunn thinks the world's fears of machine learning are drastically overblown, and he compares the intelligence of modern AI to that of a housecat. He's not concerned about AI's malicious intent, and he's not convinced that artificial general intelligence (AGI) is anywhere on the horizon.

In direct opposition to Prof. Hinton, Prof. LeCunn doesn't think LLMs have the slightest understanding of what they're analyzing. “You can manipulate language and not be smart, and that’s basically what LLMs are demonstrating.”

I fully agree with Prof. LeCunn. The transformers underlying LLMs process text using linear algebra and probabilistic computation, and have no "understanding" of the tokens they read or generate. And like guns and nuclear weapons, machine learning is only as dangerous as the people wielding them.