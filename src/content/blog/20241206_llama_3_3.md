---
title: 'Release of Llama 3.3'
slug: 'llama_3_3'
description: 'New Llama Release: version 3.3'
pubDate: 'December 6, 2024'
---

You wouldn't know it from reading their [Github site](https://github.com/meta-llama/llama-models), but Meta just released version 3.3 of their Llama series of models. Unlike previous versions, the release consists of a single model: Llama 3.3 70B Instruct. The model card is [here](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/) and you can download it [here](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct).

I haven't seen any benchmarks from reputable sites, but the early reviews have been *stellar*. Llama 3.3 keeps the 128k context of previous models, and provides enhanced tool calling with multilingual support. It outperforms Llama 3.1 70B and provides performance that compares well with Llama 405B. 

I find it interesting that Meta chose HuggingFace as their first outlet. In the Llama 3.2 release, Meta wanted developers to deploy models with their [Llama Stack API](https://github.com/meta-llama/llama-stack). I did my best to get Llama Stack working, but I never succeeded. Maybe Meta has accepted that the majority of developers access models through HuggingFace.

The timing of the release is also noteworthy. AWS announced their [Nova models](https://aws.amazon.com/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/) three days ago and OpenAI announced their [OpenAI o1 model](https://openai.com/index/introducing-chatgpt-pro/) yesterday. Is Meta trying to steal their thunder? Or are they reminding developers that open-source models are just as good, if not better than, proprietary models? Either way, my money is on Llama.